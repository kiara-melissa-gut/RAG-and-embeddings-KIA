{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        print(file)\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter, in this case [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/),  to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model (see [SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html#sentencetransformerstokentextsplitter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('faiss'):\n",
    "    os.makedirs('faiss')\n",
    "    \n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"KI wÃ¤hrend der Bachelorarbeit\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Welche hilfsmittel sind erlaubt?'  # Replace with your actual query text if needed\n",
    "original_query_text = 'Wieviele Seiten muss die Arbeit sein?'  # Replace with your actual query text if needed\n",
    "\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Task: Semantic Retrieval-Augmented Question Answering Using Groq LLM\n",
    "\n",
    "## Objective\n",
    "Implement a question-answering system that:\n",
    "1. Retrieves the most semantically relevant text passages to a user query.\n",
    "2. Constructs a natural language prompt based on the retrieved content.\n",
    "3. Uses a large language model (LLM) hosted by Groq to generate an answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Task Breakdown\n",
    "\n",
    "### 1. Embedding-Based Semantic Retrieval\n",
    "- Use the `SentenceTransformer` model `\"Sahajtomar/German-semantic\"` to encode a user query into a dense vector embedding.\n",
    "- Perform a nearest-neighbor search in a prebuilt FAISS index to retrieve the top-**k** similar text chunks. You can **use the prebuilt FAISS form above**.\n",
    "\n",
    "\n",
    "### 2. LLM Prompt Construction and Query Answering\n",
    "- Build the prompt:\n",
    "  - Using the retrieved text chunks, concatenates the results into a context block.\n",
    "  - Builds a **prompt** asking the LLM to answer the question using that context.\n",
    "  - Sends the prompt to the **Groq LLM API** (`llama-3.3-70b-versatile`) and returns the response.\n",
    "\n",
    "### 3. User Query Execution\n",
    "- An example query (`\"What is the most important factor in diagnosing asthma?\"`) is used to demonstrate the pipeline.\n",
    "- The final answer from the LLM is printed.\n",
    "\n",
    "\n",
    "## Tools & Models Used\n",
    "- **SentenceTransformers** (`Sahajtomar/German-semantic`) for embedding generation.\n",
    "- **FAISS** for efficient vector similarity search.\n",
    "- **Groq LLM API** (`llama-3.3-70b-versatile`) for generating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv(dotenv_path=\"env/.env\") #API-Key wird aus Datei aus dem Ordner env geladen\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding-Modell laden\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"Sahajtomar/German-semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielhafte Chunks fÃ¼r semantische Suche (manuell erstellt)\n",
    "# Diese dienen zur Demonstration der Pipeline und simulieren typische Inhalte aus medizinischen Texten zum Thema Asthma.\n",
    "# In einer realen Anwendung wÃ¼rden diese Chunks aus einem medizinischen Fachtext oder PDF extrahiert.\n",
    "token_split_texts = [\n",
    "    \"Asthma ist eine chronische EntzÃ¼ndung der Atemwege.\",\n",
    "    \"Typische Symptome sind Kurzatmigkeit, Husten und pfeifende Atmung.\",\n",
    "    \"Die Diagnose erfolgt meist durch eine LungenfunktionsprÃ¼fung.\",\n",
    "    \"Ein wichtiger Faktor ist die Anamnese durch den behandelnden Arzt.\",\n",
    "    \"Auch Allergietests kÃ¶nnen Hinweise liefern.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Index vorbereiten\n",
    "import faiss\n",
    "text_embeddings = model.encode(token_split_texts, convert_to_numpy=True)\n",
    "dimension = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion fÃ¼r semantisches QA Ã¼ber Groq LLM\n",
    "\n",
    "# Initialisierung desGroq-Client mit dem API-SchlÃ¼ssel aus der .env-Datei\n",
    "from groq import Groq\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "def answer_query(query, k=3):\n",
    "    # Query einbetten\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # k Ã¤hnlichsten Text-Chunks im FAISS-Index suchen\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "\n",
    "    # Prompt erzeugen mit Ã¤hnlichn Kontextpassagen\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "    prompt = (\n",
    "        \"Nutze den folgenden Kontext, um die Frage zu beantworten. \"\n",
    "        \"Wenn du die Antwort nicht im Kontext findest, sag 'Nicht im Kontext gefunden'.\\n\\n\"\n",
    "        f\"Kontext:\\n{context}\\n\\nFrage:\\n{query}\\nAntwort:\"\n",
    "    )\n",
    "\n",
    "    # Anfrage an Groq senden und Antowrt erhalten\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3-70b-8192\"\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content #generierte Antowrt zurÃ¼ckgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Query ausfÃ¼hren\n",
    "frage = \"Was ist der wichtigste Faktor bei der Diagnose von Asthma?\"\n",
    "antwort = answer_query(frage) #Funktion aufrufen mithilfe semantischer Suche --> Beantowrtung durch Groq LLM\n",
    "print(\"Antwort des Modells:\\n\", antwort) #Antwort ausgeben"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
